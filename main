import nltk
import re
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
#устанавливаем в окружение scipy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from nltk.corpus import stopwords
import keras as kr

from keras import models
from keras import layers
from keras import optimizers
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense



data_reviews_nan = pd.read_csv("data_reviews.csv",sep=";")

#удаляем nan из файла
data_reviews=data_reviews_nan.dropna()
nukk_data=data_reviews.isnull().sum(axis = 0)

# nltk.download("stopwords") # поддерживает удаление стоп-слов
# nltk.download('punkt') # делит текст на список предложений
# nltk.download('wordnet') # проводит лемматизацию
new_text = []
lemmatize = nltk.WordNetLemmatizer()

for i in data_reviews["comment"]:
    text1 = re.sub(r"[^a-zA-Z]"," ",str(i))
    text2 = nltk.word_tokenize(text1,language = "english")
    text3 = [lemmatize.lemmatize(word) for word in str(text2)]
    text4 = "".join(text3)
    new_text.append(text4)

count = CountVectorizer(stop_words="english")

matrix = count.fit_transform(new_text).toarray()

tfidf_vectorizer = TfidfVectorizer(stop_words="english")

values = tfidf_vectorizer.fit_transform(new_text)

x=matrix

y = data_reviews["emotional coloring"].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 42)

model = models.Sequential()

model.add(layers.Dense(64,activation='relu',input_shape=(66173,)))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(8, activation='relu'))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(1, activation = 'sigmoid'))

model.compile(optimizer='Adam',loss="binary_crossentropy",metrics=['accuracy'])

history=model.fit(x_train, y_train, epochs=9,batch_size=800,validation_data=(x_test,y_test))

history_dict = history.history
history_dict.keys()

# построение графика потери на этапах проверки и обучения
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(history_dict['accuracy'])+1)

plt.plot(epochs, loss_values, 'bo', label = 'Потери на этапе обучения')
plt.plot(epochs, val_loss_values, 'b', label = 'Потери на этапе проверки')
plt.title('Потери на этапах обучения и проверки')
plt.xlabel('Эпохи')
plt.ylabel('Потери')
plt.legend()
plt.show()

# построение графика точности на этапах обучения и проверки
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs, acc_values, 'bo', label = 'Точность на этапе обучения')
plt.plot(epochs, val_acc_values, 'b', label = 'Точность на этапе проверки')
plt.title('Точность на этапах обучения и проверки')
plt.xlabel('Эпохи')
plt.ylabel('Точность')
plt.legend()
plt.show()

Y_pred=model.predict(x_test)
# задаем порог 0,5 для классификации текста
Y_pred=(Y_pred>=0.5).astype("int")
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
print(classification_report(y_test,Y_pred))
print(confusion_matrix(y_test,Y_pred))

model.save('16_model_3', save_format='h5')
